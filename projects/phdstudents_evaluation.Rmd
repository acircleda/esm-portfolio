
```{r echo=FALSE, results='asis'}
project("phdstudents", 3)
```

This project marked the beginning of a new research interest of mine. Prior to this class, I had been mostly interested in issues surrounding international students in higher education, especially their academic achievement. However, I began to wonder what the carbon footprint of international student travel looked like. My readings in this area led me to discover a rich body of work on academiaâ€™s carbon footprint. Most of the research was focused on faculty with few mentions of graduate students. That gap in the research inspired this survey research project.

I developed this survey as part of EDPY 682 Educational Research. My survey was complete by the end of the course and I was excited to get IRB approval and start collecting data, which I did immediately following the course. However, I soon realized I had an instrument that had only been roughly piloted but had no rigorous examination of its validity or reliability. This forced me to rethink my study procedures and restart my project. Through this process, I learned how to conduct cognitive interviews to improve the content validity of an instrument. In addition, I learned a lot more about reliability (see Section *C. Measurement and Instrument Experiences*).

I took Survey Research after designing this survey but before deploying my second pilot to correctly collect reliability data. I am glad I did this, as I was able to take many lessons learned in that class and apply them to my survey. In addition, I was also able to modify my survey distribution plan by making sure to have repeated contacts, customized emails, and to send the survey at ideal times - concepts I picked up during the Survey Research course.  When the opportunity arose to apply for small funding through the Friends of EPC Research Grant, I decided to submit a proposal based around this project. I was successful in earning $400 in funding to be used for social media promotion of the survey (direct monetary incentives were not allowed though I would have preferred them).

The funding I received was allocated towards social media advertising (Twiiter and Higher Ed Jobs), as research showed this was a viable means of survey promotion. However, I quickly realized the difficulty of recruiting via social media, especially without fiscal incentives. Realizing I was getting a dismal response rate of less than 1%, I forumlated a new plan to recruit participants and revised the IRB to reflect recruitment via the Grad Cafe (online forum), Reddit, and by direct contact with department heads of various universities.

Direct contact with department heads was by far the most labor intensive method. It required I search for public lists of department heads, scrape  names and emails, and then contact them (using FormMule, a mail-merge program based in Googlesheets). Despite being the most labor-intensive, this was also the most fruitful method, which got me the largest number of responses. I was also presently surprised by the number of department heads who replied positively to both my request and my project idea, and were willing to help a stranger cold-emailing them. 

Unfortunately, after all funding was exhausted and over 1,500 emails were sent, the sample size for the survey remained small and I could no longer justify any further recruitment efforts, especially with no additional resources. I had to settle on a vastly different analysis plan to account for the small sample size. I also decided to incorporate the Survey of Earned Doctorates, which collects several similar data points to me own and thus allows some post-stratification weighting to improve the accuracy of my results. Analyses are complete and my manuscript is under preparation.

I had sincerely high hopes for this project, as I thought it could address a gap in the literature while at the same time informing graduate students and their advisers about where time should be more heavily invested. Instead of these positive outcomes, I have become pessimistic about survey research and the difficulty of recruiting enough participants, especially without access to ample funding. I have decided not to rely on survey research for my dissertation, as I would not want the future of my final PhD project to hinge on others' responses to a survey.

If I were to redo this survey design again, I would opt for a narrower participant pool from one field or discipline - this would allow me to target specific professional organizations as an additional recruitment method. I would have also employed some backwards design, thinking not only of the survey questions but the analytical model these questions would inform. I ran into issues of multicollinearity that were difficult to solve. These could have been avoided had I thought more about the results and the best way to analyze them. While I certainly considered which analyses I needed to use, I did not think through what the data would actually look like once in my models, and what issues could arise from that data.I think being able to simulate data and see the strengths and weaknesses of the model could have helped me construct a survey that was easier to complete and easier to analyze.

All was not lost, however. I did learn valuable lessons about survey creation and deployment. I learned the importance of designing a survey that has been piloted and vetted by experts and potential target participants (see section *C. Measurement and Instrument Experiences*). Had I known this earlier, I would not have wasted time and data with a very rough survey early in my development. This experience also taught me that survey recruitment plans need to be well-thought-out and contain multipronged approaches, especially if financial incentives are not available. Most importantly, in reflecting on this project, I see the importance of thinking about analysis and survey design simultaneously, and the need to develop models on simulated or piloted data before fully launching a survey.
