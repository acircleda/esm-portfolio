
```{r echo=FALSE, results='asis'}
project("phdstudents", 3)
```

My original design of this survey was rough - primitive even. It was not until I took Survey Research that I learned how to craft a well-designed survey that measured what was expected to be measured. One concept that was salient to me was content validity. And one method I learned to help ascertain this type of validity was cognitive interviews. I found cognitive interviews appealing because they were related to a teaching/research concept I had learned about when I was a language instructor: think alouds. In the think aloud process, students try to verbalize their thoughts as they solve a problem, decode a passage, and so on. Cognitive interviews are very similar. They allow you to get an "inside look" into the survey-takers mind, helping you understand how they interpret and answer your questions. I conducted three cognitive interviews and they were invaluable for improving my survey. This is likely one of the best practical methods I learned about in Survey Research and one I will definitley employ in future projects.


Reliability of my instrument was also a concern of mine. The importance of validity and reliability has been expressed in a number of ESM courses and I have taken that importance to heart. If I want to do good science, I need to make sure my measures are valid and reliable. However, I ran into a small problem with my survey, as it was not fit for traditional measures of reliability. The survey instrument I designed was basically a tool to collect information from participantsâ€™ CVs. It did not survey attitudes, opinions, or beliefs and thus no underlying constructs were measured. This made assessing reliability through traditional means (e.g. factor analysis) a bit more difficult. I eventually decided that what I wanted to ensure was that the questions were clear in eliciting similar types of information from all respondents. To assess the clarity of these questions, I found that test-retest reliability was appropriate and delved into the related literature I was surprised to read how many different ways to measure test-retest reliability there were, and how many of the ways were also connected with inter-rater reliability. I was also surprised at the different ways to calculate inter-rater reliability, especially the numerous different intra-class correlation coefficient models. Based on recommendations from @koo2016guideline, I chose a two-way mixed effects, absolute agreement, multiple measurement model, as it was the best fit for test-retest reliability. I subsequently generated simulation data in R and learned how to code this using the `car` and `psych` packages in R and learned how to assess test-retest reliability. Having this code already written made the comparison of first-wave and second-wave survey responses (taken twice by the same participants) a breeze.

As I wrote in Section A about this project, being able to simulate data would have made designing my survey so much easier. Testing out the reliability method using simulated data was my first ever experience using simulations. While it was not a sophisticated attempt, I think the experience was really important in helping me think through what data could look like and what needs to be done with the data. While I did not have the forethought nor skills to apply this simulated data approach to the survey design, I believe this first simulation experience helped solidify its importance in my head and will give me another tool for future projects.
