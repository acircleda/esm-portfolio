
```{r echo=FALSE, results='asis'}
project("cse", 3)
```

I met the client (Dr. Joshua Rosenberg, TPTE) at the beginning of the 2019 Fall semester. We connected over Twitter through a shared interest in R, data science, and both being at UTK. When I heard about a grant he had been awarded for developing a professional development (PD) program to help K-12 teachers prepare for teaching computer science, I asked if I could apply my growing evaluation skills (I was enrolled in Program Evaluation I at the time) to assist his project. He was more than happy to have me help. We decided that in order to create an effective PD program, we would need to understand the needs of K-12 teachers. The needs analysis for this program was largely crafted under my direction (see Appendix A for needs analysis and program logic model).

Throughout this evaluation, I learned what it was like to create a refined plan from what was merely an idea. When I joined the project, the only thing that had been done was a proposal for the funding - with very lofty goals included - and a brainstormed list of survey questions. I took that proposal and list, worked with the client to understand the goals of the project and who potential participants were, and created a survey in Qualtrics that was developed around central evaluation questions and designed under best principles for survey design (I was also enrolled in Survey Research at this time). By developing a survey that responded to client and program needs, I was able to put into practice many of the key concepts I was learning in my courses at the time.

I also learned many useful data analysis skills through this project. I used this project as a means of boosting my skills in R. This was the first time I worked with survey data in R. I learned how to import data directly into R using Qualtricsâ€™ API and the `qualtRics` package. I learned how to produce an R Markdown document, work with interactive tables using the `DT` package, version control via GitHub, and how to create an RPub. In fact, this project was the catalayst for me becoming proficient in these skills and later being able to teach them myself (see *F. Teaching and Supervisory Experience*).

Data analysis and write up occurred during the Spring 2020 semester (while I was enrolled in Program Evaluation II). Several deliverables were created for this including a presentation of preliminary results (presented at the TN STEM Conference in Cookville, TN), a technical report, and an online, one-page summary ([link](https://rpubs.com/acircleda/CSNeedsResults)). From the results, I was about to suggest concrete ideas for developing PD sessions, and an additional formative evaluation/research plan using a Kirkpatrick model to evaluate those sessions.

One important lesson I learned from the data analysis was the power of qualitative data. I typically prefer to work with quantitative data. However, for this project, I found the open-ended questions of the survey yielded more valuable information than the quantitative questions. While coding and analysis took more time, most of the suggestions for this project came from the open-ended responses. If we were to revise the survey, I would suggest including more open-ended questions to address the evaluation questions.

I was very fortunate in that the client really valued what I brought to the team in terms of rigor (the term they used often) in implementing an evaluation plan. Nevertheless, there were some struggles and issues. While I directed a majority of the needs analysis survey design, a last-minute  decision changed most of my 5-point Likert-scale questions to 3-point questions without my knowledge. This was not a major setback but felt I lost nuanced meaning when analyzing responses. In addition, I did not have control over who the survey reached. I suggested getting approval from various school districts in East Tennessee, but for time's sake, we used an already established list called the East Tennessee STEM Hub. This introduced some bias into the project, as the participants were those already interested in STEM education, and possibly computer science. Additionally, the results ended up being from across Tennessee rather than just East Tennessee.

Although I provided some concrete suggestions for designing PD sessions, based on the initial plans shared for PD workshops, it was not clear if there was any uptake of the suggestions. Specifically, I suggested a focus on raising awareness of standards with specific examples of activities to reach those standards, resources that point to already created materials (if available), and most importantly, integrating CSE with other subjects. It seems these might be included, but not at a satisfactory level. In addition, I recommended having the workshops lead by experienced teachers drawn from the public school community (as was planned in the seed funding proposal and logic model). The proposed workshops would be lead by the client himself or others from non-public school contexts. Unfortunately, due to COVID-19, the PD sessions were postponed, so neither the sessions not the Kirkpatrick evaluation plan were never implemented.

In the end, I learned a number of evaluation design, data analysis, and client relations skills. I feel this project strengthened me as both a researcher and an evaluator, especially in terms of needs analysis and utilizing qualitative survey results.
