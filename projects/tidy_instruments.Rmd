
```{r echo=FALSE, results='asis'}
project("tidy", 3)
```

Previously, I had worked with Dr. Rosenberg (TPTE) on extracting and analyzing #TidyTuesday participants' visualization code (see section *A. Evaluation and Applied Research Experience*). Initially, we examined who participated in #TidyTuesday and how their code usage changed over time. This was interesting; however, it told us very little about how their visualization skills changed over time. We wondered whether sustained participation in #TidyTuesday actually improved participants' data visualizations.

We attempted to assess this by building a survey. The survey was developed through a multi-phase, year-long process. We decided to create a crowd-sourced survey in which survey respondents would rate #TidyTuesday participants' initial, middle, and final contributions to #TidyTuesday in the defined 6-month period. We developed a list of users who had sustained participation (6 times or more over a 6-month period) and contacted them for permission to use their visualization in our survey. Knowing that getting survey respondents to rate dozens of graphics would be unrealistic, we employed a planned missing data design in which users would be shown a random set of graphics, with the goal of having a large enough sample size to have each graphic at each time point rated multiple times. 

We also used prior research to develop an efficient, 3 item rating scale for respondents to assess the graphics. We used a content validity study [@rubio2003objectifying] to make sure the scale made sense. We had a panel of experts rate the survey items, calculated a content validity index, and used this score along with open-ended feedback to revise survey items.

This was a very intensive survey design process. This was the first time I had worked with a planned missing design, a content validity study (I learned about this in Advanced Educational Measurement, which I was enrolled in during the survey design period), and randomization. What's more, we also had to consider several psychological aspects of the survey itself. While I have learned how to write good questions, how to organize a survey, and question types, this was the first time I had to think about how one question affects the next, especially since the survey task is to rate the quality of visualizations. Clearly, visuals would be judged against one another. We decided to develop two calibration graphics, one purposefully designed to be a poor data visualization; the other designed to be a good data visualization. We showed these side-by-side in an example in order to give each graphic equal viewing time (and priming time) for the participant. We then randomized their display order during the rating process. All of these efforts were meant to mitigate any primacy effects.


We ran a short pilot version of this survey to determine the optimal number of random graphics to show in order to get an adequate missing-to-complete data ratio (less than 50% missing). I simulated data following the planned missing design and wrote a special function to test various sample size and item combinations. This helped us decide on the final number of graphics to display and the sample size needed (minimum 200). We revised our item pool and randomization numbers and launched our survey.

Previous experiences I had with the need for simulated data no doubt led me to employ it in this project. I don't think a survey should be designed without simulating some of the design or generating test responses. This is important not only to develop analysis steps and a statistical models prior to data collection but to also make sure the responses are actually informative.

The biggest issue with this project was the amount of time it has taken to go from concept to reality. It has never been a number one priority for any involved, and thus, it was worked on sporadically since the project began. This is understandable. However, if we had spent a dedicated chunk of time on it rather than working on it piecemeal, I believe the project would have taken less time than it did. The quality may have also improved as we would not have kept the project on the "back burner" and constantly need a refresher of what we had done and why. I am of the opinion now that if a project is important enough to start, it is important enough to finish in a reasonable amount of time.

Overall, while I learned several new things through this survey (e.g. about randomization and planned missing designs), I felt this survey project allowed reinforcement of previous concepts I have learned related to survey design, reliability, and validity. In fact, I was adamant about making sure we test for reliability and validity prior to fully launching the survey, something that sadly does not happen often. This is  something that has been stressed in our program over and over again and I was happy to make sure this fundamental idea of measurement is embodied as much as possible in my work.

Unfortunately, this project also led to my continued negative perceptions of survey work. After all of this effort, amounting to roughly a year, we launched the survey on Twitter, with many likes and retweets. It was tagged to be visible to highly active data science communities on Twitter. We also asked that data science influencers we were familiar with share it with their followers. Despite these efforts, our sample size was minuscule, about 30 after several weeks of promotion. I've used the words disheartening and discouraging to describe past failed survey recruitment efforts. This one was likewise dispiriting, compounded by the fact that it was promoted by people on Twitter who have thousands of followers. This has served only to fortify my resolve to rarely rely on survey research unless I have greater assurances for participation (through captive audiences, incentives, etc.). 

